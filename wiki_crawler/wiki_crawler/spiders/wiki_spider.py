import scrapy
from bs4 import BeautifulSoup
import os


class WikiSpider(scrapy.Spider):
    """
    A Scrapy spider to crawl a SPECIFIC list of Wikipedia pages.

    This spider reads from 'urls.txt' and does NOT follow any
    links on the pages it crawls. This prevents topic drift.
    """

    # 1. Spider configuration
    name = 'wiki_spider'
    allowed_domains = ['en.wikipedia.org']

    # Path to the file containing the list of URLs to crawl
    # Prefer the project-level path ".../wiki_crawler/urls.txt" (generated by get_links.py),
    # but also support a fallback at ".../wiki_crawler/wiki_crawler/urls.txt".
    _SPIDER_DIR = os.path.dirname(__file__)
    _PKG_DIR = os.path.dirname(_SPIDER_DIR)  # .../wiki_crawler/wiki_crawler
    _OUTER_DIR = os.path.dirname(_PKG_DIR)   # .../wiki_crawler

    _PRIMARY_URL_FILE = os.path.join(_OUTER_DIR, 'urls.txt')
    _FALLBACK_URL_FILE = os.path.join(_PKG_DIR, 'urls.txt')

    # Choose the first existing path
    URL_FILE = _PRIMARY_URL_FILE if os.path.exists(_PRIMARY_URL_FILE) else _FALLBACK_URL_FILE

    # 2. Custom Settings
    custom_settings = {
        'USER_AGENT': 'RAGWikiCrawler/2.0 (samarthsingh02.official@gmail.com)'
    }

    def _load_urls(self):
        """Load URLs from the resolved URL_FILE path, with clear logging."""
        try:
            if not os.path.exists(self.URL_FILE):
                # Be explicit about both checked locations to aid debugging
                self.logger.error(
                    f"FATAL: URL file not found. Checked: {self._PRIMARY_URL_FILE} and {self._FALLBACK_URL_FILE}"
                )
                self.logger.error("Please create 'wiki_crawler/urls.txt' (preferred) or place it next to the spider package.")
                return []

            with open(self.URL_FILE, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]

            if not urls:
                self.logger.error(f"URL file is empty: {self.URL_FILE}")
                return []

            self.logger.info(f"Loaded {len(urls)} URLs to crawl from: {self.URL_FILE}")
            return urls
        except Exception as e:
            self.logger.error(f"Error reading URL file {self.URL_FILE}: {e}")
            return []

    def _iter_requests(self, urls):
        """Yield Scrapy Requests for the provided URLs."""
        for url in urls:
            yield scrapy.Request(url, callback=self.parse_article)

    # New API for Scrapy >= 2.13: prefer start() coroutine
    async def start(self):  # Scrapy will call this when present (async supported)
        urls = self._load_urls()
        for req in self._iter_requests(urls):
            yield req

    # Backward-compatibility for older Scrapy: keep start_requests()
    def start_requests(self):
        urls = self._load_urls()
        for req in self._iter_requests(urls):
            yield req

    def parse_article(self, response):
        """
        This method is called for every page.
        It extracts text and the last modified timestamp.
        (This logic is identical to your previous version).
        """

        # 1. Extract the clean paragraph text
        paragraphs = response.css('div#mw-content-text p::text').getall()
        all_paragraph_text = ' '.join(p.strip() for p in paragraphs if p.strip())

        # 2. Extract the "last modified" timestamp
        soup = BeautifulSoup(response.body, 'html.parser')
        last_mod_element = soup.select_one('li#footer-info-lastmod')
        last_modified_text = ""
        if last_mod_element:
            last_modified_text = last_mod_element.get_text(strip=True)

        # 3. Yield the enhanced data
        if all_paragraph_text:
            yield {
                'url': response.url,
                'text': all_paragraph_text,
                'last_modified': last_modified_text
            }